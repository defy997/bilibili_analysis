"""
æ•°æ®å¤„ç†æœåŠ¡æ¨¡å—
åŒ…å«æ•°æ®çˆ¬å–ã€æ•°æ®æ¸…æ´—ã€æƒ…æ„Ÿåˆ†æç­‰é€»è¾‘
"""
import re
import datetime
import unicodedata
import requests
from bs4 import BeautifulSoup
from django.utils import timezone
from .sentiment_model import SentimentModel
from .models import Video, Comment, Danmu, UserConfig

# åˆå§‹åŒ–OpenCCç¹ç®€è½¬æ¢
try:
    from opencc import OpenCC
    cc = OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“
    print("OpenCCåŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"OpenCCåŠ è½½å¤±è´¥: {e}, ç¹ç®€è½¬æ¢åŠŸèƒ½å°†è¢«ç¦ç”¨")
    cc = None

# åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹
try:
    MODEL_PATH = r"D:\code\python\bert-model-train\checkpoints_hotel_finetuned\best_model_epoch_3.pt"
    analyze = SentimentModel(MODEL_PATH)
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    analyze = None


# ============================================
# æ•°æ®æ¸…æ´—æ¨¡å— - é˜¶æ®µä¸€
# ============================================

def normalize_unicode(text):
    """
    Unicodeæ ‡å‡†åŒ–ï¼šç»Ÿä¸€å…¨è§’/åŠè§’å­—ç¬¦
    """
    if not text:
        return ""

    # 1. Unicodeè§„èŒƒåŒ–ä¸ºNFCå½¢å¼
    text = unicodedata.normalize('NFC', text)

    # 2. å…¨è§’è½¬åŠè§’ï¼ˆæ•°å­—ã€å­—æ¯ã€å¸¸ç”¨ç¬¦å·ï¼‰
    result = []
    for char in text:
        code = ord(char)
        # å…¨è§’ç©ºæ ¼å•ç‹¬å¤„ç†
        if code == 0x3000:
            result.append(' ')
        # å…¨è§’å­—ç¬¦ï¼ˆé™¤ç©ºæ ¼ï¼‰èŒƒå›´æ˜¯ 0xFF01-0xFF5E
        elif 0xFF01 <= code <= 0xFF5E:
            result.append(chr(code - 0xFEE0))
        else:
            result.append(char)

    return ''.join(result)


def convert_traditional_to_simplified(text):
    """
    OpenCCç¹ä½“è½¬ç®€ä½“
    """
    if not text or cc is None:
        return text

    try:
        return cc.convert(text)
    except Exception as e:
        print(f"ç¹ç®€è½¬æ¢å¤±è´¥: {e}")
        return text


def remove_emoji(text):
    """
    ç§»é™¤emojiè¡¨æƒ…ç¬¦å·ï¼ˆæ”¯æŒUnicode emojiï¼‰
    """
    if not text:
        return ""

    # ç§»é™¤emojiè¡¨æƒ…çš„æ­£åˆ™è¡¨è¾¾å¼
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œå›¾æ ‡
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # æ——å¸œ
        "\U00002702-\U000027B0"  # è£…é¥°ç¬¦å·
        "\U000024C2-\U000024FF"  # å°é—­å­—æ¯æ•°å­—
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œå›¾æ ‡
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•ç¬¦å·
        "\U0001F004-\U0001F004"  # éº»å°†ğŸ€„
        "]+",
        flags=re.UNICODE
    )

    text = emoji_pattern.sub('', text)

    # ç§»é™¤Bç«™è‡ªå¸¦è¡¨æƒ… [xxx]
    text = re.sub(r'\[.*?\]', '', text)

    return text


def compress_repeated_chars(text, max_repeat=3):
    """
    å‹ç¼©é‡å¤å­—ç¬¦
    ä¾‹å¦‚ï¼š"å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ" -> "å“ˆå“ˆå“ˆ"
    """
    if not text:
        return ""

    result = []
    count = 1
    prev_char = ''

    for char in text:
        if char == prev_char:
            count += 1
            if count <= max_repeat:
                result.append(char)
        else:
            result.append(char)
            count = 1
            prev_char = char

    return ''.join(result)


def clean_text(text, for_analysis=False):
    """
    å¢å¼ºç‰ˆæ•°æ®æ¸…æ´—ï¼šå»é™¤æ— ç”¨å­—ç¬¦ï¼Œæ ‡å‡†åŒ–æ–‡æœ¬

    Args:
        text: åŸå§‹æ–‡æœ¬
        for_analysis: æ˜¯å¦ç”¨äºæƒ…æ„Ÿåˆ†æï¼ˆTrueæ—¶ä¿ç•™æ›´å¤šè¯­ä¹‰ä¿¡æ¯ï¼‰
    """
    if not text:
        return ""

    # 1. Unicodeæ ‡å‡†åŒ–
    text = normalize_unicode(text)

    # 2. ç¹ä½“è½¬ç®€ä½“
    text = convert_traditional_to_simplified(text)

    # 3. å»é™¤URL
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

    # 4. å»é™¤@ç”¨æˆ·å
    text = re.sub(r'@[\w\u4e00-\u9fff]+', '', text)

    # 5. å»é™¤è¯é¢˜æ ‡ç­¾ #xxx#
    text = re.sub(r'#[\w\u4e00-\u9fff]+#', '', text)

    # 6. å»é™¤emojiè¡¨æƒ…
    text = remove_emoji(text)

    # 7. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n\s*', '\n', text)
    text = re.sub(r'\n+', '\n', text)  # å¤šä¸ªæ¢è¡Œç¬¦å‹ç¼©ä¸ºä¸€ä¸ª

    # 8. å»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
    if not for_analysis:
        # ç”¨äºå±•ç¤ºæ—¶ï¼Œå»é™¤æ›´å¤šæ— æ„ä¹‰ç¬¦å·
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹\[\],.!?\-]', '', text)
    else:
        # ç”¨äºåˆ†ææ—¶ï¼Œä¿ç•™å¯èƒ½æœ‰æƒ…æ„Ÿå€¾å‘çš„ç¬¦å·
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹\[\],.!?\-~â€¦]', '', text)

    # 9. å‹ç¼©é‡å¤å­—ç¬¦
    text = compress_repeated_chars(text, max_repeat=3)

    # 10. å»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()

    return text


# ============================================
# æ•°æ®è¿‡æ»¤æ¨¡å—
# ============================================

def filter_by_length(text, min_length=None, max_length=None):
    """
    é•¿åº¦è¿‡æ»¤ï¼ˆä»æ•°æ®åº“è¯»å–é»˜è®¤é…ç½®ï¼‰
    """
    if not text:
        return False

    # ä»æ•°æ®åº“è¯»å–é…ç½®
    if min_length is None or max_length is None:
        try:
            config = UserConfig.get_config()
            if min_length is None:
                min_length = config.min_length
            if max_length is None:
                max_length = config.max_length
        except:
            # æ•°æ®åº“è¯»å–å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
            min_length = min_length or 1
            max_length = max_length or 500

    length = len(text)
    return min_length <= length <= max_length


def is_spam_content(text):
    """
    åƒåœ¾å†…å®¹æ£€æµ‹ï¼ˆä»æ•°æ®åº“è¯»å–é…ç½®ï¼‰
    """
    if not text:
        return True

    # ä»æ•°æ®åº“è¯»å–é…ç½®
    try:
        config = UserConfig.get_config()
        max_char_repeat = config.max_char_repeat
        min_unique_ratio = config.min_unique_ratio
        min_unique_check_length = config.min_unique_check_length
    except:
        # æ•°æ®åº“è¯»å–å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
        max_char_repeat = 10
        min_unique_ratio = 0.2
        min_unique_check_length = 15

    # 1. çº¯æ•°å­—
    if text.isdigit():
        return True

    # 2. çº¯ç¬¦å·ï¼ˆæ²¡æœ‰ä¸­è‹±æ–‡å­—ç¬¦ï¼‰
    if not re.search(r'[\w\u4e00-\u9fff]', text):
        return True

    # 3. å•å­—ç¬¦é‡å¤ï¼ˆå¦‚ï¼šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šï¼‰
    if len(set(text)) == 1 and len(text) > max_char_repeat:
        return True

    # 4. å­—ç¬¦ç§ç±»å¤ªå°‘ï¼ˆå¯èƒ½æ˜¯æ— æ„ä¹‰å†…å®¹ï¼‰
    unique_ratio = len(set(text)) / len(text)
    if len(text) > min_unique_check_length and unique_ratio < min_unique_ratio:
        return True

    return False


def get_chinese_ratio(text):
    """
    è®¡ç®—ä¸­æ–‡å­—ç¬¦å æ¯”
    """
    if not text:
        return 0

    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    return chinese_chars / len(text)


def is_meaningful_text(text, min_chinese_ratio=None, like_count=0, reply_count=0):
    """
    ç»¼åˆåˆ¤æ–­æ–‡æœ¬æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆå¸¦ç™½åå•æœºåˆ¶ï¼Œä»æ•°æ®åº“è¯»å–é…ç½®ï¼‰

    Args:
        text: æ–‡æœ¬å†…å®¹
        min_chinese_ratio: æœ€å°ä¸­æ–‡å æ¯”ï¼ˆNone åˆ™ä»é…ç½®è¯»å–ï¼‰
        like_count: ç‚¹èµæ•°
        reply_count: å›å¤æ•°/å­è¯„è®ºæ•°

    ç™½åå•è§„åˆ™ï¼ˆæ»¡è¶³ä»»ä¸€æ¡ä»¶ç›´æ¥ä¿ç•™ï¼‰ï¼š
        1. ç‚¹èµæ•° >= high_like_threshold â†’ é«˜èµè¯„è®ºç›´æ¥ä¿ç•™
        2. å›å¤æ•° >= high_reply_threshold â†’ çƒ­é—¨è®¨è®ºè¯„è®ºç›´æ¥ä¿ç•™
        3. ç‚¹èµæ•° >= combined_like_threshold ä¸” å›å¤æ•° >= combined_reply_threshold â†’ ç»¼åˆçƒ­åº¦é«˜çš„è¯„è®ºä¿ç•™
    """
    if not text:
        return False

    # ä»æ•°æ®åº“è¯»å–é…ç½®
    try:
        config = UserConfig.get_config()
        if min_chinese_ratio is None:
            min_chinese_ratio = config.min_chinese_ratio
        high_like = config.high_like_threshold
        high_reply = config.high_reply_threshold
        combined_like = config.combined_like_threshold
        combined_reply = config.combined_reply_threshold
    except:
        # æ•°æ®åº“è¯»å–å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
        if min_chinese_ratio is None:
            min_chinese_ratio = 0.15
        high_like = 50
        high_reply = 10
        combined_like = 20
        combined_reply = 5

    # ã€ç™½åå•æœºåˆ¶ã€‘é«˜èµæˆ–é«˜å›å¤è¯„è®ºç›´æ¥é€šè¿‡
    if like_count >= high_like:
        return True  # é«˜èµè¯„è®º
    if reply_count >= high_reply:
        return True  # çƒ­é—¨è®¨è®º
    if like_count >= combined_like and reply_count >= combined_reply:
        return True  # ç»¼åˆçƒ­åº¦é«˜

    # 1. é•¿åº¦æ£€æŸ¥
    if not filter_by_length(text):
        return False

    # 2. åƒåœ¾å†…å®¹æ£€æŸ¥
    if is_spam_content(text):
        return False

    # 3. ä¸­æ–‡å æ¯”æ£€æŸ¥ï¼ˆBç«™ä¸»è¦æ˜¯ä¸­æ–‡å†…å®¹ï¼‰
    chinese_ratio = get_chinese_ratio(text)
    if chinese_ratio < min_chinese_ratio and len(text) > 10:
        # çŸ­æ–‡æœ¬å¯ä»¥å®¹å¿ä½ä¸­æ–‡å æ¯”ï¼ˆå¯èƒ½æ˜¯è‹±æ–‡æˆ–æ•°å­—ï¼‰
        return False

    return True


# ============================================
# é˜¶æ®µäºŒï¼šå»é‡æ¨¡å—
# ============================================

def text_hash(text):
    """
    è®¡ç®—æ–‡æœ¬å“ˆå¸Œå€¼ï¼ˆç”¨äºç²¾ç¡®å»é‡ï¼‰
    """
    import hashlib
    if not text:
        return ""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def exact_dedup(text_list, metadata_list=None):
    """
    æ–‡æœ¬ç²¾ç¡®å»é‡

    Args:
        text_list: æ–‡æœ¬åˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¦‚ç‚¹èµæ•°ã€æ—¶é—´ç­‰ï¼‰ï¼Œç”¨äºé€‰æ‹©ä¿ç•™å“ªä¸ª

    Returns:
        unique_indices: ä¿ç•™çš„æ–‡æœ¬ç´¢å¼•åˆ—è¡¨
        duplicate_groups: {hash: [indices]} é‡å¤ç»„
    """
    if not text_list:
        return [], {}

    hash_groups = {}  # {hash: [indices]}

    # æŒ‰å“ˆå¸Œåˆ†ç»„
    for i, text in enumerate(text_list):
        if not text:
            continue
        h = text_hash(text)
        if h not in hash_groups:
            hash_groups[h] = []
        hash_groups[h].append(i)

    # é€‰æ‹©ä¿ç•™çš„ç´¢å¼•
    unique_indices = []
    duplicate_groups = {}

    for h, indices in hash_groups.items():
        if len(indices) == 1:
            # å”¯ä¸€æ–‡æœ¬ï¼Œç›´æ¥ä¿ç•™
            unique_indices.append(indices[0])
        else:
            # é‡å¤æ–‡æœ¬ï¼Œé€‰æ‹©è´¨é‡æœ€é«˜çš„
            duplicate_groups[h] = indices

            if metadata_list:
                # æ ¹æ®å…ƒæ•°æ®é€‰æ‹©æœ€ä½³
                best_idx = indices[0]
                best_score = metadata_list[best_idx] if best_idx < len(metadata_list) else 0

                for idx in indices[1:]:
                    score = metadata_list[idx] if idx < len(metadata_list) else 0
                    if score > best_score:
                        best_score = score
                        best_idx = idx

                unique_indices.append(best_idx)
            else:
                # é»˜è®¤ä¿ç•™ç¬¬ä¸€ä¸ª
                unique_indices.append(indices[0])

    return sorted(unique_indices), duplicate_groups


def fuzzy_dedup(text_list, threshold=0.85, metadata_list=None):
    """
    æ¨¡ç³Šå»é‡ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰

    Args:
        text_list: æ–‡æœ¬åˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è§†ä¸ºé‡å¤
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¦‚ç‚¹èµæ•°ï¼‰ï¼Œç”¨äºé€‰æ‹©ä¿ç•™å“ªä¸ª

    Returns:
        unique_indices: ä¿ç•™çš„æ–‡æœ¬ç´¢å¼•åˆ—è¡¨
        similar_groups: [[indices]] ç›¸ä¼¼ç»„åˆ—è¡¨
    """
    try:
        from difflib import SequenceMatcher
    except ImportError:
        print("è­¦å‘Š: difflibä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡ç³Šå»é‡")
        return list(range(len(text_list))), []

    if not text_list:
        return [], []

    n = len(text_list)
    is_duplicate = [False] * n
    similar_groups = []
    unique_indices = []

    for i in range(n):
        if is_duplicate[i] or not text_list[i]:
            continue

        # å½“å‰æ–‡æœ¬çš„ç›¸ä¼¼ç»„
        current_group = [i]

        # ä¸åç»­æ–‡æœ¬æ¯”è¾ƒ
        for j in range(i + 1, n):
            if is_duplicate[j] or not text_list[j]:
                continue

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = SequenceMatcher(None, text_list[i], text_list[j]).ratio()

            if similarity >= threshold:
                current_group.append(j)
                is_duplicate[j] = True

        # å¦‚æœæœ‰ç›¸ä¼¼æ–‡æœ¬
        if len(current_group) > 1:
            similar_groups.append(current_group)

            # é€‰æ‹©è´¨é‡æœ€é«˜çš„
            if metadata_list:
                best_idx = current_group[0]
                best_score = metadata_list[best_idx] if best_idx < len(metadata_list) else 0

                for idx in current_group[1:]:
                    score = metadata_list[idx] if idx < len(metadata_list) else 0
                    if score > best_score:
                        best_score = score
                        best_idx = idx

                unique_indices.append(best_idx)
            else:
                unique_indices.append(current_group[0])
        else:
            unique_indices.append(i)

    return sorted(unique_indices), similar_groups


# ============================================
# é˜¶æ®µäºŒï¼šè´¨é‡è¯„åˆ†æ¨¡å—
# ============================================

def calculate_quality_score(text, like_count=0, chinese_ratio=None, min_length=5, max_length=200):
    """
    è®¡ç®—æ–‡æœ¬è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼‰

    è¯„åˆ†ç»´åº¦ï¼š
    1. é•¿åº¦åˆç†æ€§ï¼ˆ30%ï¼‰ï¼šè¿‡çŸ­æˆ–è¿‡é•¿éƒ½æ‰£åˆ†
    2. ç‚¹èµæ•°ï¼ˆ30%ï¼‰ï¼šå½’ä¸€åŒ–åçš„ç‚¹èµæ•°
    3. ä¸­æ–‡å æ¯”ï¼ˆ20%ï¼‰ï¼šä¸­æ–‡å†…å®¹è´¨é‡æ›´é«˜
    4. å†…å®¹æœ‰æ„ä¹‰æ€§ï¼ˆ20%ï¼‰ï¼šä¸æ˜¯åƒåœ¾å†…å®¹

    Args:
        text: æ–‡æœ¬å†…å®¹
        like_count: ç‚¹èµæ•°
        chinese_ratio: ä¸­æ–‡å æ¯”ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨è®¡ç®—ï¼‰
        min_length: ç†æƒ³æœ€å°é•¿åº¦
        max_length: ç†æƒ³æœ€å¤§é•¿åº¦

    Returns:
        float: è´¨é‡å¾—åˆ†ï¼ˆ0-1ï¼‰
    """
    if not text:
        return 0.0

    score = 0.0

    # 1. é•¿åº¦åˆç†æ€§å¾—åˆ†ï¼ˆ30%ï¼‰
    text_len = len(text)
    if text_len < 2:
        length_score = 0.0
    elif text_len < min_length:
        length_score = text_len / min_length * 0.7  # è¿‡çŸ­æ‰£åˆ†
    elif text_len <= max_length:
        length_score = 1.0  # ç†æƒ³é•¿åº¦
    else:
        # è¿‡é•¿æ‰£åˆ†
        excess = text_len - max_length
        length_score = max(0.5, 1.0 - excess / max_length)

    score += length_score * 0.3

    # 2. ç‚¹èµæ•°å¾—åˆ†ï¼ˆ30%ï¼‰
    # ä½¿ç”¨å¯¹æ•°å½’ä¸€åŒ–ï¼Œé¿å…æç«¯å€¼å½±å“
    import math
    if like_count > 0:
        # log(1+x)å½’ä¸€åŒ–ï¼Œå‡è®¾100èµä¸ºæ»¡åˆ†
        like_score = min(1.0, math.log(1 + like_count) / math.log(101))
    else:
        like_score = 0.1  # 0èµç»™åŸºç¡€åˆ†

    score += like_score * 0.3

    # 3. ä¸­æ–‡å æ¯”å¾—åˆ†ï¼ˆ20%ï¼‰
    if chinese_ratio is None:
        chinese_ratio = get_chinese_ratio(text)

    # ä¸­æ–‡å æ¯”è¶Šé«˜å¾—åˆ†è¶Šé«˜ï¼Œä½†çº¯è‹±æ–‡ä¹Ÿç»™åŸºç¡€åˆ†
    if chinese_ratio >= 0.5:
        chinese_score = 1.0
    elif chinese_ratio >= 0.3:
        chinese_score = 0.8
    elif chinese_ratio > 0:
        chinese_score = 0.6
    else:
        chinese_score = 0.4  # çº¯è‹±æ–‡/æ•°å­—

    score += chinese_score * 0.2

    # 4. å†…å®¹æœ‰æ„ä¹‰æ€§å¾—åˆ†ï¼ˆ20%ï¼‰
    if is_spam_content(text):
        meaningful_score = 0.0
    else:
        # å­—ç¬¦å¤šæ ·æ€§
        unique_ratio = len(set(text)) / len(text)
        meaningful_score = min(1.0, unique_ratio * 2)  # å¤šæ ·æ€§è¶Šé«˜è¶Šå¥½

    score += meaningful_score * 0.2

    return round(score, 3)


# ============================================
# é˜¶æ®µä¸‰ï¼šEmbeddingè¯­ä¹‰å»é‡æ¨¡å—
# ============================================

# å…¨å±€Embeddingæ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_embedding_model = None


def load_embedding_model(model_name='paraphrase-multilingual-MiniLM-L12-v2'):
    """
    åŠ è½½Embeddingæ¨¡å‹ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰

    æ¨èæ¨¡å‹ï¼š
    - paraphrase-multilingual-MiniLM-L12-v2: è½»é‡çº§å¤šè¯­è¨€æ¨¡å‹ï¼ˆæ¨èï¼‰
    - distiluse-base-multilingual-cased-v2: æ›´å¤§çš„å¤šè¯­è¨€æ¨¡å‹
    """
    global _embedding_model

    if _embedding_model is not None:
        return _embedding_model

    try:
        from sentence_transformers import SentenceTransformer
        print(f"æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {model_name}")
        _embedding_model = SentenceTransformer(model_name)
        print("Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
        return _embedding_model
    except ImportError:
        print("é”™è¯¯: æœªå®‰è£…sentence-transformersï¼Œè¯·è¿è¡Œ: pip install sentence-transformers")
        return None
    except Exception as e:
        print(f"Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None


def generate_embeddings(text_list, model=None, batch_size=32):
    """
    ç”Ÿæˆæ–‡æœ¬å‘é‡

    Args:
        text_list: æ–‡æœ¬åˆ—è¡¨
        model: Embeddingæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        numpy.ndarray: æ–‡æœ¬å‘é‡çŸ©é˜µ (n_texts, embedding_dim)
    """
    if not text_list:
        return None

    if model is None:
        model = load_embedding_model()

    if model is None:
        return None

    try:
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text if text else "" for text in text_list]
        embeddings = model.encode(valid_texts, batch_size=batch_size, show_progress_bar=True)
        return embeddings
    except Exception as e:
        print(f"ç”ŸæˆEmbeddingå¤±è´¥: {e}")
        return None


def embedding_dedup(text_list, threshold=0.85, metadata_list=None, batch_size=32):
    """
    åŸºäºEmbeddingçš„è¯­ä¹‰å»é‡

    Args:
        text_list: æ–‡æœ¬åˆ—è¡¨
        threshold: ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è§†ä¸ºè¯­ä¹‰é‡å¤
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¦‚ç‚¹èµæ•°ï¼‰ï¼Œç”¨äºé€‰æ‹©ä¿ç•™å“ªä¸ª
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        unique_indices: ä¿ç•™çš„æ–‡æœ¬ç´¢å¼•åˆ—è¡¨
        similar_groups: [[indices]] è¯­ä¹‰ç›¸ä¼¼ç»„åˆ—è¡¨
    """
    if not text_list:
        return [], []

    # ç”Ÿæˆembeddings
    embeddings = generate_embeddings(text_list, batch_size=batch_size)

    if embeddings is None:
        print("è­¦å‘Š: Embeddingç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡è¯­ä¹‰å»é‡")
        return list(range(len(text_list))), []

    try:
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
    except ImportError:
        print("é”™è¯¯: æœªå®‰è£…scikit-learnï¼Œè¯·è¿è¡Œ: pip install scikit-learn")
        return list(range(len(text_list))), []

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = cosine_similarity(embeddings)

    n = len(text_list)
    is_duplicate = [False] * n
    similar_groups = []
    unique_indices = []

    for i in range(n):
        if is_duplicate[i]:
            continue

        # å½“å‰æ–‡æœ¬çš„ç›¸ä¼¼ç»„
        current_group = [i]

        # æŸ¥æ‰¾ä¸å½“å‰æ–‡æœ¬ç›¸ä¼¼çš„å…¶ä»–æ–‡æœ¬
        for j in range(i + 1, n):
            if is_duplicate[j]:
                continue

            if similarity_matrix[i][j] >= threshold:
                current_group.append(j)
                is_duplicate[j] = True

        # å¦‚æœæœ‰è¯­ä¹‰ç›¸ä¼¼æ–‡æœ¬
        if len(current_group) > 1:
            similar_groups.append(current_group)

            # é€‰æ‹©è´¨é‡æœ€é«˜çš„
            if metadata_list:
                best_idx = current_group[0]
                best_score = metadata_list[best_idx] if best_idx < len(metadata_list) else 0

                for idx in current_group[1:]:
                    score = metadata_list[idx] if idx < len(metadata_list) else 0
                    if score > best_score:
                        best_score = score
                        best_idx = idx

                unique_indices.append(best_idx)
            else:
                unique_indices.append(current_group[0])
        else:
            unique_indices.append(i)

    print(f"è¯­ä¹‰å»é‡: {n}æ¡ -> {len(unique_indices)}æ¡ï¼Œå»é™¤{n - len(unique_indices)}æ¡é‡å¤")

    return sorted(unique_indices), similar_groups


# ============================================
# é˜¶æ®µä¸‰ï¼šå®Œæ•´Pipelineç±»
# ============================================

class DataCleaningPipeline:
    """
    æ•°æ®æ¸…æ´—Pipelineç±»

    åŠŸèƒ½ï¼š
    1. æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–
    2. æ•°æ®è¿‡æ»¤
    3. è´¨é‡è¯„åˆ†
    4. å¤šç§å»é‡æ–¹å¼
    5. æ¸…æ´—æŠ¥å‘Šå’Œç»Ÿè®¡

    ç”¨æ³•ï¼š
        pipeline = DataCleaningPipeline()
        pipeline.add_texts(text_list, metadata_list)
        pipeline.clean()
        pipeline.filter()
        pipeline.deduplicate(method='embedding')
        results = pipeline.get_results()
    """

    def __init__(self, config=None):
        """
        åˆå§‹åŒ–Pipelineï¼ˆä»æ•°æ®åº“è¯»å–é»˜è®¤é…ç½®ï¼‰

        Args:
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œç”¨äºè¦†ç›–é»˜è®¤é…ç½®ï¼‰
        """
        # ä»æ•°æ®åº“è¯»å–é»˜è®¤é…ç½®
        try:
            db_config = UserConfig.get_config()
            self.config = {
                'clean_for_analysis': False,
                'min_length': db_config.min_length,
                'max_length': db_config.max_length,
                'min_chinese_ratio': db_config.min_chinese_ratio,
                'min_quality_score': db_config.min_quality_score,
                'dedup_method': db_config.dedup_method,
                'fuzzy_threshold': db_config.fuzzy_threshold,
                'embedding_threshold': db_config.embedding_threshold,
            }
        except:
            # æ•°æ®åº“è¯»å–å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
            self.config = {
                'clean_for_analysis': False,
                'min_length': 1,
                'max_length': 500,
                'min_chinese_ratio': 0.15,
                'min_quality_score': 0.2,
                'dedup_method': 'exact',
                'fuzzy_threshold': 0.85,
                'embedding_threshold': 0.85,
            }

        # æ›´æ–°é…ç½®ï¼ˆå¦‚æœæä¾›äº†è‡ªå®šä¹‰é…ç½®ï¼‰
        if config:
            self.config.update(config)

        # æ•°æ®å­˜å‚¨
        self.original_texts = []
        self.cleaned_texts = []
        self.metadata = []
        self.quality_scores = []
        self.valid_indices = []  # é€šè¿‡æ‰€æœ‰è¿‡æ»¤çš„ç´¢å¼•

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'original_count': 0,
            'after_clean': 0,
            'after_filter': 0,
            'after_dedup': 0,
            'removed_by_length': 0,
            'removed_by_spam': 0,
            'removed_by_chinese_ratio': 0,
            'removed_by_quality': 0,
            'removed_by_dedup': 0,
            'duplicate_groups': [],
        }

    def add_texts(self, text_list, metadata_list=None):
        """
        æ·»åŠ å¾…å¤„ç†æ–‡æœ¬

        Args:
            text_list: æ–‡æœ¬åˆ—è¡¨
            metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¦‚ç‚¹èµæ•°ã€æ—¶é—´ç­‰ï¼‰
        """
        self.original_texts = text_list
        self.stats['original_count'] = len(text_list)

        if metadata_list:
            self.metadata = metadata_list
        else:
            self.metadata = [0] * len(text_list)

        return self

    def clean(self):
        """
        æ–‡æœ¬æ¸…æ´—æ­¥éª¤
        """
        print("å¼€å§‹æ–‡æœ¬æ¸…æ´—...")

        for_analysis = self.config['clean_for_analysis']
        self.cleaned_texts = [
            clean_text(text, for_analysis=for_analysis)
            for text in self.original_texts
        ]

        self.stats['after_clean'] = len(self.cleaned_texts)
        print(f"æ¸…æ´—å®Œæˆ: {self.stats['original_count']}æ¡æ–‡æœ¬")

        return self

    def filter(self):
        """
        æ•°æ®è¿‡æ»¤æ­¥éª¤
        """
        print("å¼€å§‹æ•°æ®è¿‡æ»¤...")

        filtered_indices = []

        for i, text in enumerate(self.cleaned_texts):
            # é•¿åº¦è¿‡æ»¤
            if not filter_by_length(text, self.config['min_length'], self.config['max_length']):
                self.stats['removed_by_length'] += 1
                continue

            # åƒåœ¾å†…å®¹è¿‡æ»¤
            if is_spam_content(text):
                self.stats['removed_by_spam'] += 1
                continue

            # ä¸­æ–‡å æ¯”è¿‡æ»¤
            chinese_ratio = get_chinese_ratio(text)
            if len(text) > 10 and chinese_ratio < self.config['min_chinese_ratio']:
                self.stats['removed_by_chinese_ratio'] += 1
                continue

            filtered_indices.append(i)

        self.valid_indices = filtered_indices
        self.stats['after_filter'] = len(self.valid_indices)

        print(f"è¿‡æ»¤å®Œæˆ: {self.stats['after_filter']}/{self.stats['original_count']}æ¡é€šè¿‡")

        return self

    def calculate_quality(self):
        """
        è®¡ç®—è´¨é‡è¯„åˆ†
        """
        print("è®¡ç®—è´¨é‡è¯„åˆ†...")

        self.quality_scores = []

        for i in self.valid_indices:
            text = self.cleaned_texts[i]
            like_count = self.metadata[i] if i < len(self.metadata) else 0

            score = calculate_quality_score(
                text,
                like_count=like_count,
                min_length=self.config['min_length'],
                max_length=self.config['max_length']
            )

            self.quality_scores.append(score)

        # æŒ‰è´¨é‡åˆ†è¿‡æ»¤
        min_score = self.config['min_quality_score']
        before_count = len(self.valid_indices)

        filtered_indices = []
        filtered_scores = []

        for i, score in enumerate(self.quality_scores):
            if score >= min_score:
                filtered_indices.append(self.valid_indices[i])
                filtered_scores.append(score)
            else:
                self.stats['removed_by_quality'] += 1

        self.valid_indices = filtered_indices
        self.quality_scores = filtered_scores

        print(f"è´¨é‡è¯„åˆ†å®Œæˆ: {len(self.valid_indices)}/{before_count}æ¡è¾¾æ ‡")

        return self

    def deduplicate(self, method=None):
        """
        å»é‡æ­¥éª¤

        Args:
            method: å»é‡æ–¹æ³•
                - 'exact': ç²¾ç¡®å»é‡
                - 'fuzzy': æ¨¡ç³Šå»é‡
                - 'embedding': è¯­ä¹‰å»é‡
                - 'all': ä¾æ¬¡åº”ç”¨æ‰€æœ‰æ–¹æ³•
        """
        if method is None:
            method = self.config['dedup_method']

        print(f"å¼€å§‹å»é‡ (æ–¹æ³•: {method})...")

        # è·å–æœ‰æ•ˆæ–‡æœ¬å’Œå…ƒæ•°æ®
        valid_texts = [self.cleaned_texts[i] for i in self.valid_indices]
        valid_metadata = [self.metadata[i] for i in self.valid_indices]

        before_count = len(valid_texts)

        if method == 'exact':
            unique_indices, dup_groups = exact_dedup(valid_texts, valid_metadata)
            self.stats['duplicate_groups'].extend(dup_groups.values())

        elif method == 'fuzzy':
            threshold = self.config['fuzzy_threshold']
            unique_indices, sim_groups = fuzzy_dedup(valid_texts, threshold, valid_metadata)
            self.stats['duplicate_groups'].extend(sim_groups)

        elif method == 'embedding':
            threshold = self.config['embedding_threshold']
            unique_indices, sim_groups = embedding_dedup(valid_texts, threshold, valid_metadata)
            self.stats['duplicate_groups'].extend(sim_groups)

        elif method == 'all':
            # ä¾æ¬¡åº”ç”¨æ‰€æœ‰å»é‡æ–¹æ³•
            print("  - åº”ç”¨ç²¾ç¡®å»é‡...")
            unique_indices, dup_groups = exact_dedup(valid_texts, valid_metadata)
            self.stats['duplicate_groups'].extend(dup_groups.values())

            # æ›´æ–°æ–‡æœ¬åˆ—è¡¨
            valid_texts = [valid_texts[i] for i in unique_indices]
            valid_metadata = [valid_metadata[i] for i in unique_indices]

            print("  - åº”ç”¨æ¨¡ç³Šå»é‡...")
            threshold = self.config['fuzzy_threshold']
            unique_indices2, sim_groups = fuzzy_dedup(valid_texts, threshold, valid_metadata)
            self.stats['duplicate_groups'].extend(sim_groups)

            # æ˜ å°„å›åŸå§‹ç´¢å¼•
            unique_indices = [unique_indices[i] for i in unique_indices2]

            # æ›´æ–°æ–‡æœ¬åˆ—è¡¨
            valid_texts = [valid_texts[i] for i in unique_indices2]
            valid_metadata = [valid_metadata[i] for i in unique_indices2]

            print("  - åº”ç”¨è¯­ä¹‰å»é‡...")
            threshold = self.config['embedding_threshold']
            unique_indices3, sim_groups = embedding_dedup(valid_texts, threshold, valid_metadata)
            self.stats['duplicate_groups'].extend(sim_groups)

            # æ˜ å°„å›åŸå§‹ç´¢å¼•
            unique_indices = [unique_indices[i] for i in unique_indices3]

        else:
            print(f"æœªçŸ¥çš„å»é‡æ–¹æ³•: {method}")
            return self

        # æ›´æ–°æœ‰æ•ˆç´¢å¼•
        self.valid_indices = [self.valid_indices[i] for i in unique_indices]
        self.quality_scores = [self.quality_scores[i] for i in unique_indices] if self.quality_scores else []

        self.stats['removed_by_dedup'] = before_count - len(self.valid_indices)
        self.stats['after_dedup'] = len(self.valid_indices)

        print(f"å»é‡å®Œæˆ: {self.stats['after_dedup']}/{before_count}æ¡ä¿ç•™")

        return self

    def get_results(self):
        """
        è·å–å¤„ç†ç»“æœ

        Returns:
            {
                'texts': [æ¸…æ´—åçš„æ–‡æœ¬],
                'original_indices': [åŸå§‹ç´¢å¼•],
                'quality_scores': [è´¨é‡è¯„åˆ†],
                'metadata': [å…ƒæ•°æ®],
            }
        """
        results = {
            'texts': [self.cleaned_texts[i] for i in self.valid_indices],
            'original_indices': self.valid_indices,
            'quality_scores': self.quality_scores,
            'metadata': [self.metadata[i] for i in self.valid_indices],
        }

        return results

    def get_stats(self):
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        """
        return self.stats

    def print_report(self):
        """
        æ‰“å°æ¸…æ´—æŠ¥å‘Š
        """
        print("\n" + "=" * 60)
        print("æ•°æ®æ¸…æ´—æŠ¥å‘Š")
        print("=" * 60)
        print(f"åŸå§‹æ•°æ®: {self.stats['original_count']}æ¡")
        print(f"æ¸…æ´—å: {self.stats['after_clean']}æ¡")
        print(f"è¿‡æ»¤å: {self.stats['after_filter']}æ¡")
        print(f"å»é‡å: {self.stats['after_dedup']}æ¡")
        print("\nè¿‡æ»¤è¯¦æƒ…:")
        print(f"  - é•¿åº¦ä¸ç¬¦: {self.stats['removed_by_length']}æ¡")
        print(f"  - åƒåœ¾å†…å®¹: {self.stats['removed_by_spam']}æ¡")
        print(f"  - ä¸­æ–‡å æ¯”ä½: {self.stats['removed_by_chinese_ratio']}æ¡")
        print(f"  - è´¨é‡ä¸è¾¾æ ‡: {self.stats['removed_by_quality']}æ¡")
        print(f"  - é‡å¤å†…å®¹: {self.stats['removed_by_dedup']}æ¡")
        print(f"\næœ€ç»ˆä¿ç•™ç‡: {self.stats['after_dedup'] / self.stats['original_count'] * 100:.1f}%")
        print("=" * 60 + "\n")


def crawl_video_info(bvid, headers, cookie):
    """
    çˆ¬å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
    """
    video_url = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"
    resp = requests.get(video_url, headers=headers)
    data = resp.json()

    if data['code'] != 0:
        raise Exception(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {data['message']}")

    return {
        'aid': data["data"]["aid"],
        'cid': data['data']['cid'],
        'title': data['data']['title'],
        'pubdate_ts': data['data'].get('pubdate')
    }


def crawl_comments(aid, headers, pages=3):
    """
    çˆ¬å–è§†é¢‘è¯„è®º
    """
    all_comments = []
    for page in range(1, pages + 1):
        comment_api = "https://api.bilibili.com/x/v2/reply/main"
        params = {
            "type": 1,
            "oid": aid,
            "next": page
        }
        try:
            comment_resp = requests.get(comment_api, params=params, headers=headers)
            comment_data = comment_resp.json()

            if comment_data['code'] != 0:
                break

            replies = comment_data['data'].get('replies', [])
            if not replies:
                break

            all_comments.extend(replies)
        except Exception as e:
            print(f"è·å–ç¬¬{page}é¡µè¯„è®ºå‡ºé”™: {e}")
            break

    return all_comments


def crawl_danmaku(cid, headers):
    """
    çˆ¬å–è§†é¢‘å¼¹å¹•
    """
    danmaku_list = []
    try:
        danmaku_api = f"https://api.bilibili.com/x/v1/dm/list.so?oid={cid}"
        danmaku_resp = requests.get(danmaku_api, headers=headers)

        if danmaku_resp.status_code == 200:
            soup = BeautifulSoup(danmaku_resp.content, 'xml')
            danmaku_elements = soup.find_all('d')

            for element in danmaku_elements:
                text = element.get_text(strip=True)
                if text:
                    danmaku_list.append(text)

        print(f"è·å–åˆ° {len(danmaku_list)} æ¡å¼¹å¹•")
    except Exception as e:
        print(f"è·å–å¼¹å¹•å¤±è´¥: {e}")

    return danmaku_list


def save_video(video_info, bvid):
    """
    ä¿å­˜è§†é¢‘ä¿¡æ¯åˆ°æ•°æ®åº“
    """
    pubdate_ts = video_info.get('pubdate_ts')
    pubdate_dt = None
    if pubdate_ts:
        try:
            naive_dt = datetime.datetime.fromtimestamp(pubdate_ts)
            pubdate_dt = timezone.make_aware(naive_dt)
        except:
            pass

    video_obj, created = Video.objects.get_or_create(
        bvid=bvid,
        defaults={
            'aid': video_info['aid'],
            'cid': video_info['cid'],
            'title': video_info['title'],
            'pubdate': pubdate_dt
        }
    )
    if created:
        print(f"æ–°å»ºè§†é¢‘è®°å½•: {video_info['title']}")

    return video_obj


def save_comment(comment_data, video_obj, score, sentiment_label):
    """
    ä¿å­˜å•æ¡è¯„è®ºåˆ°æ•°æ®åº“ï¼ˆé›†æˆæ•°æ®æ¸…æ´—å’Œè¿‡æ»¤ï¼‰
    """
    try:
        # æå–å­—æ®µ
        rpid = comment_data.get('rpid')
        uname = comment_data.get('member', {}).get('uname', '')
        message = comment_data.get('content', {}).get('message', '')
        like_count = comment_data.get('like', 0)
        reply_count = comment_data.get('rcount', 0)  # å­è¯„è®ºæ•°
        mid = comment_data.get('mid', 0)
        parent_rpid = comment_data.get('parent', 0)

        # æ•°æ®æ¸…æ´—ï¼ˆç”¨äºå±•ç¤ºçš„ç‰ˆæœ¬ï¼‰
        cleaned_message = clean_text(message, for_analysis=False)

        # æ•°æ®è¿‡æ»¤ï¼šæ£€æŸ¥æ¸…æ´—åçš„æ–‡æœ¬æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆå¸¦ç™½åå•æœºåˆ¶ï¼‰
        if not is_meaningful_text(cleaned_message, like_count=like_count, reply_count=reply_count):
            print(f"è¯„è®ºè¢«è¿‡æ»¤ (rpid={rpid}): æ— æ„ä¹‰å†…å®¹ (èµ:{like_count}, å›å¤:{reply_count})")
            return None

        # æå–èº«ä»½æ ‡ç­¾
        vip_info = comment_data.get('member', {}).get('vip', {})
        vip_type = vip_info.get('vipType', 0)
        vip_label = vip_info.get('label', {}).get('text', '')

        # å¤„ç†æ—¶é—´
        ctime_ts = comment_data.get('ctime')
        ctime_dt = None
        if ctime_ts:
            try:
                naive_dt = datetime.datetime.fromtimestamp(ctime_ts)
                ctime_dt = timezone.make_aware(naive_dt)
            except:
                pass

        # IPå±åœ°
        location = comment_data.get('reply_control', {}).get('location', 'æœªçŸ¥')
        if "IPå±åœ°ï¼š" in location:
            location = location.replace("IPå±åœ°ï¼š", "")

        # ä¿å­˜
        comment, _ = Comment.objects.update_or_create(
            rpid=rpid,
            defaults={
                'video': video_obj,
                'mid': mid,
                'uname': uname,
                'message': cleaned_message,
                'like_count': like_count,
                'reply_count': reply_count,  # æ–°å¢ï¼šå­è¯„è®ºæ•°
                'location': location,
                'ctime': ctime_dt,
                'vip_type': vip_type,
                'vip_label': vip_label,
                'parent_rpid': parent_rpid,
                'sentiment_score': score,
                'sentiment_label': sentiment_label
            }
        )
        return comment
    except Exception as e:
        print(f"ä¿å­˜è¯„è®ºå¤±è´¥ (rpid={comment_data.get('rpid')}): {e}")
        return None


def save_danmaku(cid, content, score, sentiment_label):
    """
    ä¿å­˜å•æ¡å¼¹å¹•åˆ°æ•°æ®åº“ï¼ˆé›†æˆæ•°æ®æ¸…æ´—å’Œè¿‡æ»¤ï¼‰
    """
    try:
        # æ•°æ®æ¸…æ´—ï¼ˆç”¨äºå±•ç¤ºçš„ç‰ˆæœ¬ï¼‰
        cleaned_content = clean_text(content, for_analysis=False)

        # æ•°æ®è¿‡æ»¤ï¼šæ£€æŸ¥æ¸…æ´—åçš„æ–‡æœ¬æ˜¯å¦æœ‰æ„ä¹‰
        if not is_meaningful_text(cleaned_content):
            # å¼¹å¹•è¿‡æ»¤ä¸æ‰“å°æ—¥å¿—ï¼ˆæ•°é‡å¤ªå¤šï¼‰
            return None

        Danmu.objects.create(
            cid=cid,
            content=cleaned_content,
            sentiment_score=score,
            sentiment_label=sentiment_label
        )
        return True
    except Exception as e:
        print(f"ä¿å­˜å¼¹å¹•å¤±è´¥: {e}")
        return None


def get_sentiment_label(score):
    """
    æ ¹æ®å¾—åˆ†åˆ¤æ–­æƒ…æ„Ÿåˆ†ç±»
    """
    if score >= 0.6:
        return "positive"
    elif score <= 0.4:
        return "negative"
    else:
        return "neutral"


def analyze_sentiment(text_list):
    """
    æ‰¹é‡æƒ…æ„Ÿåˆ†æ
    """
    if analyze is None:
        raise Exception("æ¨¡å‹æœªåŠ è½½æˆåŠŸ")

    scores = analyze.predict(text_list)
    return scores


def process_video(bvid, headers, cookie):
    """
    å¤„ç†å•ä¸ªè§†é¢‘ï¼šçˆ¬å– -> æ¸…æ´— -> åˆ†æ -> ä¿å­˜
    """
    print(f"å¼€å§‹å¤„ç†è§†é¢‘: {bvid}")

    # 1. çˆ¬å–è§†é¢‘ä¿¡æ¯
    video_info = crawl_video_info(bvid, headers, cookie)
    video_obj = save_video(video_info, bvid)

    # 2. çˆ¬å–è¯„è®ºå’Œå¼¹å¹•
    all_comments = crawl_comments(video_info['aid'], headers)
    danmaku_list = crawl_danmaku(video_info['cid'], headers)

    # 3. æ•°æ®æ¸…æ´—å’Œè¿‡æ»¤ï¼ˆè¯„è®ºï¼‰
    # ç”¨äºåˆ†æçš„æ–‡æœ¬ï¼ˆä¿ç•™æ›´å¤šè¯­ä¹‰ä¿¡æ¯ï¼‰
    analysis_comments = []
    valid_comment_indices = []  # è®°å½•æœ‰æ•ˆè¯„è®ºçš„ç´¢å¼•

    for i, comment in enumerate(all_comments):
        message = comment.get('content', {}).get('message', '')
        if message:
            cleaned = clean_text(message, for_analysis=True)
            if is_meaningful_text(cleaned):
                analysis_comments.append(cleaned)
                valid_comment_indices.append(i)

    # 3. æ•°æ®æ¸…æ´—å’Œè¿‡æ»¤ï¼ˆå¼¹å¹•ï¼‰
    analysis_danmu = []
    valid_danmu_indices = []  # è®°å½•æœ‰æ•ˆå¼¹å¹•çš„ç´¢å¼•

    for i, content in enumerate(danmaku_list):
        if content:
            cleaned = clean_text(content, for_analysis=True)
            if is_meaningful_text(cleaned):
                analysis_danmu.append(cleaned)
                valid_danmu_indices.append(i)

    # 4. åˆå¹¶æ–‡æœ¬ç”¨äºåˆ†æ
    raw_texts = analysis_comments + analysis_danmu

    if not raw_texts:
        print("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        return {"status": "no_data"}

    print(f"åŸå§‹è¯„è®º: {len(all_comments)}, æœ‰æ•ˆè¯„è®º: {len(analysis_comments)}")
    print(f"åŸå§‹å¼¹å¹•: {len(danmaku_list)}, æœ‰æ•ˆå¼¹å¹•: {len(analysis_danmu)}")

    # 5. æƒ…æ„Ÿåˆ†æ
    scores = analyze_sentiment(raw_texts)

    # 6. ä¿å­˜ç»“æœ
    # ä¿å­˜è¯„è®ºï¼ˆä½¿ç”¨æœ‰æ•ˆè¯„è®ºçš„ç´¢å¼•ï¼‰
    comment_count = 0
    for analysis_idx, original_idx in enumerate(valid_comment_indices):
        comment = all_comments[original_idx]
        score = scores[analysis_idx] if analysis_idx < len(scores) else 0.5
        sentiment = get_sentiment_label(score)
        result = save_comment(comment, video_obj, score, sentiment)
        if result:
            comment_count += 1

    # ä¿å­˜å¼¹å¹•ï¼ˆä½¿ç”¨æœ‰æ•ˆå¼¹å¹•çš„ç´¢å¼•ï¼‰
    danmu_count = 0
    base_idx = len(analysis_comments)  # å¼¹å¹•çš„å¾—åˆ†ä»è¯„è®ºä¹‹åå¼€å§‹
    for analysis_idx, original_idx in enumerate(valid_danmu_indices):
        content = danmaku_list[original_idx]
        score_idx = base_idx + analysis_idx
        score = scores[score_idx] if score_idx < len(scores) else 0.5
        sentiment = get_sentiment_label(score)
        result = save_danmaku(video_info['cid'], content, score, sentiment)
        if result:
            danmu_count += 1

    # 7. ç»Ÿè®¡ç»“æœ
    positive_count = sum(1 for s in scores if s >= 0.6)
    negative_count = sum(1 for s in scores if s <= 0.4)
    neutral_count = len(scores) - positive_count - negative_count

    return {
        "status": "success",
        "title": video_info['title'],
        "positive_count": positive_count,
        "neutral_count": neutral_count,
        "negative_count": negative_count,
        "comment_count": comment_count,
        "danmu_count": danmu_count
    }

