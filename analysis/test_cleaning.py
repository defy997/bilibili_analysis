"""
æ•°æ®æ¸…æ´—åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•é˜¶æ®µä¸€çš„å„é¡¹åŠŸèƒ½
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bilibili_analysis.settings')
import django
django.setup()

from analysis.services import (
    normalize_unicode,
    convert_traditional_to_simplified,
    remove_emoji,
    compress_repeated_chars,
    clean_text,
    filter_by_length,
    is_spam_content,
    get_chinese_ratio,
    is_meaningful_text
)


def test_normalize_unicode():
    """æµ‹è¯•Unicodeè§„èŒƒåŒ–"""
    print("\n=== æµ‹è¯•Unicodeè§„èŒƒåŒ– ===")
    test_cases = [
        ("ï¼¨ï½…ï½Œï½Œï½ã€€ï¼·ï½ï½’ï½Œï½„", "Hello World"),
        ("ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "1234567890"),
        ("ï¼ï¼ ï¼ƒï¿¥ï¼…", "!@#Â¥%"),
    ]
    for original, expected in test_cases:
        result = normalize_unicode(original)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} '{original}' -> '{result}'")


def test_convert_traditional():
    """æµ‹è¯•ç¹ç®€è½¬æ¢"""
    print("\n=== æµ‹è¯•ç¹ç®€è½¬æ¢ ===")
    test_cases = [
        "è³‡æ–™æ¸…æ´—æ¸¬è©¦",
        "é€™å€‹è¦–é »çœŸä¸éŒ¯",
        "ç¹é«”è½‰ç°¡é«”",
    ]
    for text in test_cases:
        result = convert_traditional_to_simplified(text)
        print(f"'{text}' -> '{result}'")


def test_remove_emoji():
    """æµ‹è¯•emojiç§»é™¤"""
    print("\n=== æµ‹è¯•Emojiç§»é™¤ ===")
    test_cases = [
        "å“ˆå“ˆå“ˆğŸ˜‚ğŸ˜‚ğŸ˜‚",
        "å¤ªå¥½äº†ğŸ‘ğŸ‘ğŸ‘",
        "[doge]è¿™ä¸ªè¡¨æƒ…",
        "ğŸ‰ğŸ‰åº†ç¥ä¸€ä¸‹ğŸ‰ğŸ‰",
    ]
    for text in test_cases:
        result = remove_emoji(text)
        print(f"'{text}' -> '{result}'")


def test_compress_repeated():
    """æµ‹è¯•é‡å¤å­—ç¬¦å‹ç¼©"""
    print("\n=== æµ‹è¯•é‡å¤å­—ç¬¦å‹ç¼© ===")
    test_cases = [
        "å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ",
        "å¤ªå¤ªå¤ªå¤ªå¤ªå¥½äº†",
        "ï¼ï¼ï¼ï¼ï¼ï¼",
    ]
    for text in test_cases:
        result = compress_repeated_chars(text, max_repeat=3)
        print(f"'{text}' -> '{result}'")


def test_clean_text():
    """æµ‹è¯•å®Œæ•´æ¸…æ´—æµç¨‹"""
    print("\n=== æµ‹è¯•å®Œæ•´æ¸…æ´—æµç¨‹ ===")
    test_cases = [
        "ã€è¦–é »ã€‘é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼https://bilibili.com @ç”¨æˆ·å #è¯é¢˜#",
        "ï¼¨ï½…ï½Œï½Œï½ã€€ï¼·ï½ï½’ï½Œï½„ï¼ï¼ï¼é€™æ˜¯ç¹é«”å­—ğŸ˜‚ğŸ˜‚ğŸ˜‚",
        "[doge][doge]å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ@someone #tag#",
        "   å¤šä½™ç©ºæ ¼    å’Œ\n\n\næ¢è¡Œç¬¦   ",
    ]

    print("\n--- ç”¨äºå±•ç¤ºçš„æ¸…æ´— ---")
    for text in test_cases:
        result = clean_text(text, for_analysis=False)
        print(f"åŸæ–‡: {text}")
        print(f"ç»“æœ: {result}\n")

    print("\n--- ç”¨äºåˆ†æçš„æ¸…æ´— ---")
    for text in test_cases:
        result = clean_text(text, for_analysis=True)
        print(f"åŸæ–‡: {text}")
        print(f"ç»“æœ: {result}\n")


def test_filters():
    """æµ‹è¯•æ•°æ®è¿‡æ»¤"""
    print("\n=== æµ‹è¯•æ•°æ®è¿‡æ»¤ ===")

    print("\n--- é•¿åº¦è¿‡æ»¤ ---")
    test_cases = [
        ("a", False),
        ("æ­£å¸¸é•¿åº¦çš„è¯„è®º", True),
        ("è¿™" + "æ˜¯" * 300, False),  # è¶…é•¿
    ]
    for text, expected in test_cases:
        result = filter_by_length(text)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} '{text[:20]}...' -> {result} (é¢„æœŸ: {expected})")

    print("\n--- åƒåœ¾å†…å®¹æ£€æµ‹ ---")
    test_cases = [
        ("12345678", True),  # çº¯æ•°å­—
        ("ï¼ï¼ï¼@#ï¿¥%", True),  # çº¯ç¬¦å·
        ("å•Šå•Šå•Šå•Šå•Šå•Šå•Š", True),  # å•å­—ç¬¦é‡å¤
        ("æ­£å¸¸çš„è¯„è®ºå†…å®¹", False),
        ("aaaaaaaabbbb", True),  # å­—ç¬¦ç§ç±»å¤ªå°‘
    ]
    for text, expected in test_cases:
        result = is_spam_content(text)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} '{text}' -> åƒåœ¾:{result} (é¢„æœŸ: {expected})")

    print("\n--- ä¸­æ–‡å æ¯” ---")
    test_cases = [
        "è¿™æ˜¯ä¸­æ–‡è¯„è®º",
        "Hello World",
        "ä¸­è‹±æ··åˆ Mixed Content",
        "666666",
    ]
    for text in test_cases:
        ratio = get_chinese_ratio(text)
        print(f"'{text}' -> ä¸­æ–‡å æ¯”: {ratio:.2%}")

    print("\n--- ç»¼åˆæœ‰æ„ä¹‰æ£€æµ‹ ---")
    test_cases = [
        ("è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„è¯„è®º", True),
        ("a", False),  # å¤ªçŸ­
        ("12345678", False),  # çº¯æ•°å­—
        ("ï¼ï¼ï¼ï¼ï¼", False),  # çº¯ç¬¦å·
        ("Hello World Test", True),  # è‹±æ–‡ä¹Ÿå¯ä»¥
        ("å•Šå•Šå•Šå•Šå•Šå•Šå•Š", False),  # é‡å¤
    ]
    for text, expected in test_cases:
        result = is_meaningful_text(text)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} '{text}' -> æœ‰æ„ä¹‰:{result} (é¢„æœŸ: {expected})")


def test_dedup():
    """æµ‹è¯•å»é‡åŠŸèƒ½"""
    print("\n=== æµ‹è¯•å»é‡åŠŸèƒ½ï¼ˆé˜¶æ®µäºŒï¼‰ ===")

    from analysis.services import exact_dedup, fuzzy_dedup, text_hash

    print("\n--- æ–‡æœ¬å“ˆå¸Œ ---")
    texts = ["è¿™æ˜¯æµ‹è¯•", "è¿™æ˜¯æµ‹è¯•", "è¿™æ˜¯ä¸åŒçš„æ–‡æœ¬"]
    for text in texts:
        h = text_hash(text)
        print(f"'{text}' -> {h[:16]}...")

    print("\n--- ç²¾ç¡®å»é‡ ---")
    test_texts = [
        "è¿™æ˜¯é‡å¤çš„è¯„è®º",
        "è¿™æ˜¯å¦ä¸€æ¡è¯„è®º",
        "è¿™æ˜¯é‡å¤çš„è¯„è®º",  # é‡å¤
        "å®Œå…¨ä¸åŒçš„å†…å®¹",
        "è¿™æ˜¯å¦ä¸€æ¡è¯„è®º",  # é‡å¤
    ]
    metadata = [5, 10, 3, 7, 15]  # ç‚¹èµæ•°

    unique_indices, dup_groups = exact_dedup(test_texts, metadata)
    print(f"åŸå§‹: {len(test_texts)}æ¡ï¼Œå»é‡å: {len(unique_indices)}æ¡")
    print(f"ä¿ç•™çš„ç´¢å¼•: {unique_indices}")
    print(f"é‡å¤ç»„: {len(dup_groups)}ç»„")

    print("\n--- æ¨¡ç³Šå»é‡ ---")
    test_texts2 = [
        "è¿™ä¸ªè§†é¢‘çœŸä¸é”™",
        "è¿™ä¸ªè§†é¢‘çœŸçš„ä¸é”™",  # ç›¸ä¼¼
        "å®Œå…¨ä¸åŒçš„è¯„è®º",
        "è¿™è§†é¢‘ä¸é”™å•Š",  # ç›¸ä¼¼
        "å¦ä¸€æ¡è¯„è®º",
    ]

    unique_indices, sim_groups = fuzzy_dedup(test_texts2, threshold=0.7)
    print(f"åŸå§‹: {len(test_texts2)}æ¡ï¼Œå»é‡å: {len(unique_indices)}æ¡")
    print(f"ä¿ç•™çš„ç´¢å¼•: {unique_indices}")
    print(f"ç›¸ä¼¼ç»„: {len(sim_groups)}ç»„")


def test_quality_score():
    """æµ‹è¯•è´¨é‡è¯„åˆ†"""
    print("\n=== æµ‹è¯•è´¨é‡è¯„åˆ†ï¼ˆé˜¶æ®µäºŒï¼‰ ===")

    from analysis.services import calculate_quality_score

    test_cases = [
        ("æ­£å¸¸çš„è¯„è®ºå†…å®¹ï¼Œé•¿åº¦é€‚ä¸­", 10),
        ("a", 0),  # å¤ªçŸ­
        ("è¿™æ˜¯ä¸€æ¡å¾ˆé•¿å¾ˆé•¿çš„è¯„è®º" * 50, 5),  # å¤ªé•¿
        ("12345678", 0),  # çº¯æ•°å­—
        ("éå¸¸æ£’çš„è§†é¢‘ï¼", 100),  # é«˜èµ
        ("Hello World Test", 5),  # è‹±æ–‡
    ]

    for text, like_count in test_cases:
        score = calculate_quality_score(text, like_count)
        display_text = text[:20] + "..." if len(text) > 20 else text
        print(f"'{display_text}' (èµ:{like_count}) -> è´¨é‡åˆ†: {score}")


def test_pipeline():
    """æµ‹è¯•å®Œæ•´Pipeline"""
    print("\n=== æµ‹è¯•å®Œæ•´Pipelineï¼ˆé˜¶æ®µä¸‰ï¼‰ ===")

    from analysis.services import DataCleaningPipeline

    # æ¨¡æ‹Ÿæ•°æ®
    test_texts = [
        "ã€è¦–é »ã€‘é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼ğŸ˜‚ğŸ˜‚",
        "é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼",  # ç›¸ä¼¼
        "a",  # å¤ªçŸ­
        "12345678",  # åƒåœ¾
        "è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„è¯„è®º",
        "å¦ä¸€æ¡ä¸åŒçš„è¯„è®º",
        "è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„è¯„è®º",  # é‡å¤
        "è¿˜æœ‰ä¸€æ¡è´¨é‡ä¸é”™çš„è¯„è®ºå†…å®¹",
        "ï¼ï¼ï¼ï¼ï¼",  # åƒåœ¾
        "è¿™æ¡è¯„è®ºå†…å®¹å¾ˆä¸°å¯Œï¼Œè®¨è®ºäº†å¾ˆå¤šæœ‰è¶£çš„è¯é¢˜",
    ]

    metadata = [10, 5, 0, 0, 8, 12, 3, 15, 0, 20]  # ç‚¹èµæ•°

    # åˆ›å»ºPipeline
    config = {
        'clean_for_analysis': False,
        'min_length': 2,
        'max_length': 500,
        'min_quality_score': 0.3,
        'dedup_method': 'exact',  # ä½¿ç”¨ç²¾ç¡®å»é‡ï¼ˆembeddingéœ€è¦æ¨¡å‹ï¼‰
    }

    pipeline = DataCleaningPipeline(config)

    # æ‰§è¡ŒPipeline
    pipeline.add_texts(test_texts, metadata)
    pipeline.clean()
    pipeline.filter()
    pipeline.calculate_quality()
    pipeline.deduplicate()

    # è·å–ç»“æœ
    results = pipeline.get_results()

    print(f"\nå¤„ç†ç»“æœ:")
    print(f"ä¿ç•™ {len(results['texts'])} æ¡æ•°æ®")
    print("\nä¿ç•™çš„æ–‡æœ¬:")
    for i, (text, score, orig_idx) in enumerate(zip(
        results['texts'],
        results['quality_scores'],
        results['original_indices']
    ), 1):
        print(f"  {i}. [{orig_idx}] (è´¨é‡:{score:.2f}) {text}")

    # æ‰“å°æŠ¥å‘Š
    pipeline.print_report()


if __name__ == "__main__":
    print("=" * 60)
    print("æ•°æ®æ¸…æ´—åŠŸèƒ½æµ‹è¯• - é˜¶æ®µä¸€ã€äºŒã€ä¸‰")
    print("=" * 60)

    try:
        # é˜¶æ®µä¸€æµ‹è¯•
        print("\n" + "=" * 60)
        print("é˜¶æ®µä¸€ï¼šåŸºç¡€æ¸…æ´—")
        print("=" * 60)
        test_normalize_unicode()
        test_convert_traditional()
        test_remove_emoji()
        test_compress_repeated()
        test_clean_text()
        test_filters()

        # é˜¶æ®µäºŒæµ‹è¯•
        print("\n" + "=" * 60)
        print("é˜¶æ®µäºŒï¼šå»é‡å’Œè´¨é‡è¯„åˆ†")
        print("=" * 60)
        test_dedup()
        test_quality_score()

        # é˜¶æ®µä¸‰æµ‹è¯•
        print("\n" + "=" * 60)
        print("é˜¶æ®µä¸‰ï¼šå®Œæ•´Pipeline")
        print("=" * 60)
        test_pipeline()

        print("\n" + "=" * 60)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
    except Exception as e:
        print(f"\næµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
