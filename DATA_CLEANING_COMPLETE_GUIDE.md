# æ•°æ®æ¸…æ´—æ¨¡å—å®Œæ•´æŒ‡å— - é˜¶æ®µä¸€ã€äºŒã€ä¸‰

## ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [å®‰è£…ä¾èµ–](#å®‰è£…ä¾èµ–)
- [é˜¶æ®µä¸€ï¼šåŸºç¡€æ¸…æ´—](#é˜¶æ®µä¸€åŸºç¡€æ¸…æ´—)
- [é˜¶æ®µäºŒï¼šå»é‡å’Œè´¨é‡è¯„åˆ†](#é˜¶æ®µäºŒå»é‡å’Œè´¨é‡è¯„åˆ†)
- [é˜¶æ®µä¸‰ï¼šé«˜çº§åŠŸèƒ½](#é˜¶æ®µä¸‰é«˜çº§åŠŸèƒ½)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)

---

## æ¦‚è¿°

æœ¬é¡¹ç›®å·²å®ç°å®Œæ•´çš„ä¸‰é˜¶æ®µæ•°æ®æ¸…æ´—åŠŸèƒ½ï¼š

### é˜¶æ®µä¸€ï¼šåŸºç¡€æ¸…æ´—
- âœ… Unicodeæ ‡å‡†åŒ–ï¼ˆå…¨è§’/åŠè§’è½¬æ¢ï¼‰
- âœ… OpenCCç¹ç®€è½¬æ¢
- âœ… Emojiå’Œè¡¨æƒ…ç¬¦å·æ¸…ç†
- âœ… é‡å¤å­—ç¬¦å‹ç¼©
- âœ… åŸºç¡€æ•°æ®è¿‡æ»¤ï¼ˆé•¿åº¦ã€åƒåœ¾å†…å®¹ã€ä¸­æ–‡å æ¯”ï¼‰

### é˜¶æ®µäºŒï¼šå»é‡å’Œè´¨é‡è¯„åˆ†
- âœ… æ–‡æœ¬ç²¾ç¡®å»é‡ï¼ˆåŸºäºå“ˆå¸Œï¼‰
- âœ… æ¨¡ç³Šå»é‡ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰
- âœ… è´¨é‡è¯„åˆ†ç³»ç»Ÿ

### é˜¶æ®µä¸‰ï¼šé«˜çº§åŠŸèƒ½
- âœ… Embeddingè¯­ä¹‰å»é‡ï¼ˆåŸºäºsentence-transformersï¼‰
- âœ… å®Œæ•´Pipelineç±»
- âœ… æ‰¹é‡å¤„ç†ä¼˜åŒ–

---

## å®‰è£…ä¾èµ–

### 1. å®‰è£…PythonåŒ…

```bash
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–ï¼š
- `opencc-python-reimplemented`: ç¹ç®€è½¬æ¢
- `sentence-transformers`: Embeddingæ¨¡å‹ï¼ˆè¯­ä¹‰å»é‡ï¼‰
- `scikit-learn`: ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

### 2. éªŒè¯å®‰è£…

è¿è¡Œå®Œæ•´æµ‹è¯•ï¼š

```bash
cd D:\code\python\bilibili_analysis
python analysis\test_cleaning.py
```

---

## é˜¶æ®µä¸€ï¼šåŸºç¡€æ¸…æ´—

### 1.1 æ–‡æœ¬æ ‡å‡†åŒ–

#### normalize_unicode(text)
```python
from analysis.services import normalize_unicode

text = "ï¼¨ï½…ï½Œï½Œï½ã€€ï¼·ï½ï½’ï½Œï½„"
result = normalize_unicode(text)  # "Hello World"
```

#### convert_traditional_to_simplified(text)
```python
from analysis.services import convert_traditional_to_simplified

text = "è³‡æ–™æ¸…æ´—æ¸¬è©¦"
result = convert_traditional_to_simplified(text)  # "èµ„æ–™æ¸…æ´—æµ‹è¯•"
```

#### remove_emoji(text)
```python
from analysis.services import remove_emoji

text = "å“ˆå“ˆå“ˆğŸ˜‚ğŸ˜‚ğŸ˜‚[doge]"
result = remove_emoji(text)  # "å“ˆå“ˆå“ˆ"
```

#### compress_repeated_chars(text, max_repeat=3)
```python
from analysis.services import compress_repeated_chars

text = "å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ"
result = compress_repeated_chars(text, max_repeat=3)  # "å“ˆå“ˆå“ˆ"
```

### 1.2 å®Œæ•´æ¸…æ´—å‡½æ•°

#### clean_text(text, for_analysis=False)

**ç”¨äºå±•ç¤ºï¼š**
```python
text = "ã€è¦–é »ã€‘é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼https://bilibili.com @ç”¨æˆ·å"
result = clean_text(text, for_analysis=False)
# ç»“æœ: "è¿™ä¸ªè§†é¢‘å¤ªæ£’äº†!"
```

**ç”¨äºåˆ†æï¼š**
```python
result = clean_text(text, for_analysis=True)
# ç»“æœ: "è¿™ä¸ªè§†é¢‘å¤ªæ£’äº†ï¼ï¼ï¼ï¼" (ä¿ç•™æƒ…æ„Ÿç¬¦å·)
```

### 1.3 æ•°æ®è¿‡æ»¤

#### filter_by_length(text, min_length=2, max_length=500)
```python
from analysis.services import filter_by_length

is_valid = filter_by_length("æ­£å¸¸è¯„è®º")  # True
is_valid = filter_by_length("a")  # False
```

#### is_spam_content(text)
```python
from analysis.services import is_spam_content

is_spam = is_spam_content("12345678")  # True
is_spam = is_spam_content("æ­£å¸¸è¯„è®º")  # False
```

#### is_meaningful_text(text)
```python
from analysis.services import is_meaningful_text

is_valid = is_meaningful_text("è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„è¯„è®º")  # True
is_valid = is_meaningful_text("ï¼ï¼ï¼")  # False
```

---

## é˜¶æ®µäºŒï¼šå»é‡å’Œè´¨é‡è¯„åˆ†

### 2.1 ç²¾ç¡®å»é‡

#### exact_dedup(text_list, metadata_list=None)

åŸºäºMD5å“ˆå¸Œçš„ç²¾ç¡®å»é‡ï¼Œä¿ç•™è´¨é‡æœ€é«˜ï¼ˆå¦‚ç‚¹èµæ•°æœ€å¤šï¼‰çš„æ ·æœ¬ã€‚

```python
from analysis.services import exact_dedup

texts = [
    "è¿™æ˜¯é‡å¤çš„è¯„è®º",
    "è¿™æ˜¯å¦ä¸€æ¡è¯„è®º",
    "è¿™æ˜¯é‡å¤çš„è¯„è®º",  # é‡å¤
]
metadata = [5, 10, 8]  # ç‚¹èµæ•°

unique_indices, dup_groups = exact_dedup(texts, metadata)
# unique_indices: [0, 1] (ä¿ç•™äº†ç‚¹èµæ•°æ›´é«˜çš„ç¬¬ä¸€æ¡)
# dup_groups: {hash: [0, 2]}
```

### 2.2 æ¨¡ç³Šå»é‡

#### fuzzy_dedup(text_list, threshold=0.85, metadata_list=None)

åŸºäºç¼–è¾‘è·ç¦»ï¼ˆSequenceMatcherï¼‰çš„æ¨¡ç³Šå»é‡ï¼Œè¯†åˆ«é«˜åº¦ç›¸ä¼¼çš„æ–‡æœ¬ã€‚

```python
from analysis.services import fuzzy_dedup

texts = [
    "è¿™ä¸ªè§†é¢‘çœŸä¸é”™",
    "è¿™ä¸ªè§†é¢‘çœŸçš„ä¸é”™",  # ç›¸ä¼¼
    "å®Œå…¨ä¸åŒçš„è¯„è®º",
]

unique_indices, sim_groups = fuzzy_dedup(texts, threshold=0.7)
# unique_indices: [0, 2] (åˆå¹¶äº†ç›¸ä¼¼çš„å‰ä¸¤æ¡)
# sim_groups: [[0, 1]]
```

**é˜ˆå€¼è¯´æ˜ï¼š**
- `0.9-1.0`: æé«˜ç›¸ä¼¼ï¼ˆå‡ ä¹ç›¸åŒï¼‰
- `0.8-0.9`: é«˜ç›¸ä¼¼ï¼ˆæ¨èï¼‰
- `0.7-0.8`: ä¸­ç­‰ç›¸ä¼¼
- `< 0.7`: ä½ç›¸ä¼¼ï¼ˆå¯èƒ½è¯¯åˆ¤ï¼‰

### 2.3 è´¨é‡è¯„åˆ†

#### calculate_quality_score(text, like_count=0, ...)

ç»¼åˆè¯„åˆ†ç³»ç»Ÿï¼ˆ0-1åˆ†ï¼‰ï¼Œè€ƒè™‘ï¼š
- é•¿åº¦åˆç†æ€§ï¼ˆ30%ï¼‰
- ç‚¹èµæ•°ï¼ˆ30%ï¼‰
- ä¸­æ–‡å æ¯”ï¼ˆ20%ï¼‰
- å†…å®¹æœ‰æ„ä¹‰æ€§ï¼ˆ20%ï¼‰

```python
from analysis.services import calculate_quality_score

score = calculate_quality_score("æ­£å¸¸çš„è¯„è®ºå†…å®¹", like_count=10)
# score: 0.75

score = calculate_quality_score("a", like_count=0)
# score: 0.1 (å¤ªçŸ­)

score = calculate_quality_score("éå¸¸æ£’çš„è§†é¢‘ï¼", like_count=100)
# score: 0.9+ (é«˜èµ+ä¼˜è´¨å†…å®¹)
```

---

## é˜¶æ®µä¸‰ï¼šé«˜çº§åŠŸèƒ½

### 3.1 Embeddingè¯­ä¹‰å»é‡

#### load_embedding_model(model_name='paraphrase-multilingual-MiniLM-L12-v2')

åŠ è½½å¤šè¯­è¨€Embeddingæ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰ã€‚

```python
from analysis.services import load_embedding_model

model = load_embedding_model()
# æ¨èæ¨¡å‹ï¼šparaphrase-multilingual-MiniLM-L12-v2 (è½»é‡çº§)
```

#### embedding_dedup(text_list, threshold=0.85, metadata_list=None, batch_size=32)

åŸºäºè¯­ä¹‰å‘é‡çš„å»é‡ï¼Œè¯†åˆ«è¡¨è¿°ä¸åŒä½†æ„æ€ç›¸åŒçš„å†…å®¹ã€‚

```python
from analysis.services import embedding_dedup

texts = [
    "è¿™ä¸ªè§†é¢‘çœŸä¸é”™",
    "è¿™è§†é¢‘æŒºå¥½çš„",  # è¯­ä¹‰ç›¸ä¼¼
    "å®Œå…¨ä¸åŒçš„å†…å®¹",
]

unique_indices, sim_groups = embedding_dedup(texts, threshold=0.85)
# è‡ªåŠ¨è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼çš„è¯„è®º
```

**ä¼˜åŠ¿ï¼š**
- è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¿°ä¸åŒçš„å†…å®¹
- æ”¯æŒä¸­è‹±æ–‡æ··åˆ
- é€‚åˆæ£€æµ‹æœºå™¨äººåˆ·å±ï¼ˆåŒä¹‰æ”¹å†™ï¼‰

**æ€§èƒ½ï¼š**
- é¦–æ¬¡åŠ è½½æ¨¡å‹éœ€è¦ä¸‹è½½ï¼ˆçº¦100MBï¼‰
- æ‰¹é‡å¤„ç†é€Ÿåº¦å¿«ï¼ˆæ”¯æŒGPUåŠ é€Ÿï¼‰

### 3.2 å®Œæ•´Pipelineç±»

#### DataCleaningPipeline

æ•´åˆæ‰€æœ‰æ¸…æ´—ã€è¿‡æ»¤ã€å»é‡åŠŸèƒ½çš„Pipelineç±»ã€‚

**åŸºç¡€ç”¨æ³•ï¼š**

```python
from analysis.services import DataCleaningPipeline

# åˆ›å»ºPipeline
pipeline = DataCleaningPipeline()

# æ·»åŠ æ•°æ®
texts = ["è¯„è®º1", "è¯„è®º2", ...]
metadata = [10, 5, ...]  # ç‚¹èµæ•°

pipeline.add_texts(texts, metadata)

# æ‰§è¡Œæ¸…æ´—æµç¨‹
pipeline.clean()          # æ–‡æœ¬æ¸…æ´—
pipeline.filter()         # æ•°æ®è¿‡æ»¤
pipeline.calculate_quality()  # è´¨é‡è¯„åˆ†
pipeline.deduplicate()    # å»é‡

# è·å–ç»“æœ
results = pipeline.get_results()
pipeline.print_report()   # æ‰“å°æ¸…æ´—æŠ¥å‘Š
```

**é«˜çº§é…ç½®ï¼š**

```python
config = {
    'clean_for_analysis': False,      # æ˜¯å¦ç”¨äºåˆ†æ
    'min_length': 2,                  # æœ€å°é•¿åº¦
    'max_length': 500,                # æœ€å¤§é•¿åº¦
    'min_chinese_ratio': 0.3,         # æœ€å°ä¸­æ–‡å æ¯”
    'min_quality_score': 0.3,         # æœ€å°è´¨é‡åˆ†
    'dedup_method': 'embedding',      # å»é‡æ–¹æ³•: exact/fuzzy/embedding/all
    'fuzzy_threshold': 0.85,          # æ¨¡ç³Šå»é‡é˜ˆå€¼
    'embedding_threshold': 0.85,      # è¯­ä¹‰å»é‡é˜ˆå€¼
}

pipeline = DataCleaningPipeline(config)
```

**é“¾å¼è°ƒç”¨ï¼š**

```python
results = (DataCleaningPipeline(config)
    .add_texts(texts, metadata)
    .clean()
    .filter()
    .calculate_quality()
    .deduplicate(method='all')  # åº”ç”¨æ‰€æœ‰å»é‡æ–¹æ³•
    .get_results())
```

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºç¡€æ¸…æ´—

```python
from analysis.services import clean_text, is_meaningful_text

text = "ã€è¦–é »ã€‘é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼ğŸ˜‚ğŸ˜‚"

# æ¸…æ´—
cleaned = clean_text(text, for_analysis=False)
# ç»“æœ: "è¿™ä¸ªè§†é¢‘å¤ªæ£’äº†!"

# éªŒè¯
if is_meaningful_text(cleaned):
    print("æœ‰æ•ˆè¯„è®º")
```

### ç¤ºä¾‹2ï¼šå»é‡ + è´¨é‡è¯„åˆ†

```python
from analysis.services import exact_dedup, calculate_quality_score

texts = ["è¯„è®º1", "è¯„è®º2", "è¯„è®º1"]  # æœ‰é‡å¤
likes = [10, 5, 3]

# ç²¾ç¡®å»é‡
unique_indices, _ = exact_dedup(texts, likes)

# è®¡ç®—è´¨é‡åˆ†
for idx in unique_indices:
    score = calculate_quality_score(texts[idx], likes[idx])
    print(f"{texts[idx]}: è´¨é‡åˆ† {score}")
```

### ç¤ºä¾‹3ï¼šå®Œæ•´Pipeline

```python
from analysis.services import DataCleaningPipeline

# æ¨¡æ‹ŸBç«™è¯„è®ºæ•°æ®
comments = [
    "ã€è¦–é »ã€‘é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼ğŸ˜‚ğŸ˜‚",
    "é€™å€‹è¦–é »å¤ªæ£’äº†ï¼ï¼ï¼ï¼",  # ç›¸ä¼¼
    "a",  # å¤ªçŸ­
    "è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„è¯„è®º",
    "è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„è¯„è®º",  # é‡å¤
]
likes = [10, 5, 0, 8, 3]

# åˆ›å»ºPipeline
config = {
    'min_quality_score': 0.3,
    'dedup_method': 'all',  # åº”ç”¨æ‰€æœ‰å»é‡æ–¹æ³•
}

pipeline = DataCleaningPipeline(config)

# æ‰§è¡Œ
results = (pipeline
    .add_texts(comments, likes)
    .clean()
    .filter()
    .calculate_quality()
    .deduplicate()
    .get_results())

# æŸ¥çœ‹ç»“æœ
for text, score in zip(results['texts'], results['quality_scores']):
    print(f"{text} (è´¨é‡: {score:.2f})")

# æ‰“å°æŠ¥å‘Š
pipeline.print_report()
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
æ•°æ®æ¸…æ´—æŠ¥å‘Š
============================================================
åŸå§‹æ•°æ®: 5æ¡
æ¸…æ´—å: 5æ¡
è¿‡æ»¤å: 3æ¡
å»é‡å: 2æ¡

è¿‡æ»¤è¯¦æƒ…:
  - é•¿åº¦ä¸ç¬¦: 1æ¡
  - åƒåœ¾å†…å®¹: 0æ¡
  - ä¸­æ–‡å æ¯”ä½: 0æ¡
  - è´¨é‡ä¸è¾¾æ ‡: 1æ¡
  - é‡å¤å†…å®¹: 1æ¡

æœ€ç»ˆä¿ç•™ç‡: 40.0%
============================================================
```

---

## é…ç½®è¯´æ˜

### Pipelineé…ç½®é¡¹

| é…ç½®é¡¹ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `clean_for_analysis` | bool | False | æ˜¯å¦ç”¨äºåˆ†æï¼ˆTrueä¿ç•™æ›´å¤šç¬¦å·ï¼‰ |
| `min_length` | int | 2 | æœ€å°æ–‡æœ¬é•¿åº¦ |
| `max_length` | int | 500 | æœ€å¤§æ–‡æœ¬é•¿åº¦ |
| `min_chinese_ratio` | float | 0.3 | æœ€å°ä¸­æ–‡å æ¯”ï¼ˆ0-1ï¼‰ |
| `min_quality_score` | float | 0.3 | æœ€å°è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼‰ |
| `dedup_method` | str | 'exact' | å»é‡æ–¹æ³•ï¼šexact/fuzzy/embedding/all |
| `fuzzy_threshold` | float | 0.85 | æ¨¡ç³Šå»é‡ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ |
| `embedding_threshold` | float | 0.85 | è¯­ä¹‰å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ |

### å»é‡æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | é€Ÿåº¦ | å‡†ç¡®åº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|----------|
| `exact` | æå¿« | å®Œç¾ | å®Œå…¨ç›¸åŒçš„æ–‡æœ¬ |
| `fuzzy` | å¿« | é«˜ | æ–‡å­—ç›¸ä¼¼ï¼ˆé”™åˆ«å­—ã€æ ‡ç‚¹å·®å¼‚ï¼‰ |
| `embedding` | æ…¢ | å¾ˆé«˜ | è¯­ä¹‰ç›¸ä¼¼ï¼ˆåŒä¹‰æ”¹å†™ï¼‰ |
| `all` | æ…¢ | æœ€é«˜ | ç»¼åˆå»é‡ï¼ˆæ¨èï¼‰ |

### è´¨é‡è¯„åˆ†æƒé‡

- é•¿åº¦åˆç†æ€§ï¼š30%ï¼ˆç†æƒ³5-200å­—ç¬¦ï¼‰
- ç‚¹èµæ•°ï¼š30%ï¼ˆlogå½’ä¸€åŒ–ï¼Œ100èµä¸ºæ»¡åˆ†ï¼‰
- ä¸­æ–‡å æ¯”ï¼š20%ï¼ˆä¸­æ–‡>=50%ä¸ºæ»¡åˆ†ï¼‰
- å†…å®¹æœ‰æ„ä¹‰æ€§ï¼š20%ï¼ˆå­—ç¬¦å¤šæ ·æ€§ï¼‰

---

## é›†æˆåˆ°çˆ¬è™«æµç¨‹

æ•°æ®æ¸…æ´—å·²è‡ªåŠ¨é›†æˆåˆ°`process_video()`å‡½æ•°ï¼š

```python
from analysis.services import process_video

result = process_video(
    bvid="BV1xx411c7XZ",
    headers=headers,
    cookie=cookie
)

print(f"å¤„ç†å®Œæˆ: {result['comment_count']}æ¡è¯„è®ºï¼Œ{result['danmu_count']}æ¡å¼¹å¹•")
```

**è‡ªåŠ¨å¤„ç†æµç¨‹ï¼š**
1. çˆ¬å–åŸå§‹æ•°æ®
2. æ¸…æ´—ï¼ˆé˜¶æ®µä¸€ï¼‰
3. è¿‡æ»¤æ— æ•ˆæ•°æ®
4. æƒ…æ„Ÿåˆ†æ
5. ä¿å­˜åˆ°æ•°æ®åº“

**æ³¨æ„ï¼š** Pipelineæ–¹å¼éœ€è¦æ‰‹åŠ¨è°ƒç”¨ï¼Œä¸ä¼šè‡ªåŠ¨é›†æˆåˆ°çˆ¬è™«æµç¨‹ã€‚

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹é‡å¤„ç†
```python
# ä½¿ç”¨Pipelineæ‰¹é‡å¤„ç†å¤§é‡æ•°æ®
pipeline = DataCleaningPipeline()
pipeline.add_texts(large_text_list, metadata_list)
pipeline.clean().filter().deduplicate(method='exact')  # å…ˆç”¨å¿«é€Ÿæ–¹æ³•
```

### 2. é€‰æ‹©åˆé€‚çš„å»é‡æ–¹æ³•
- å°æ•°æ®é‡ï¼ˆ<1000ï¼‰ï¼šä½¿ç”¨ `all`
- ä¸­ç­‰æ•°æ®é‡ï¼ˆ1000-10000ï¼‰ï¼šä½¿ç”¨ `fuzzy`
- å¤§æ•°æ®é‡ï¼ˆ>10000ï¼‰ï¼šä½¿ç”¨ `exact`

### 3. Embeddingæ¨¡å‹ä¼˜åŒ–
```python
# é¦–æ¬¡ä½¿ç”¨ä¼šä¸‹è½½æ¨¡å‹ï¼Œå»ºè®®æå‰åŠ è½½
from analysis.services import load_embedding_model
model = load_embedding_model()  # é¢„åŠ è½½

# å¦‚æœæœ‰GPUï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨GPUåŠ é€Ÿ
```

---

## å¸¸è§é—®é¢˜

### Q1: OpenCCåŠ è½½å¤±è´¥ï¼Ÿ
**A:** æ£€æŸ¥å®‰è£…ï¼š
```bash
pip install opencc-python-reimplemented
```

### Q2: Embeddingæ¨¡å‹ä¸‹è½½æ…¢ï¼Ÿ
**A:** æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `~/.cache/huggingface/`ï¼Œå¯ä»¥ï¼š
1. ä½¿ç”¨å›½å†…é•œåƒ
2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q3: å¦‚ä½•è°ƒæ•´è¿‡æ»¤ä¸¥æ ¼åº¦ï¼Ÿ
**A:** ä¿®æ”¹Pipelineé…ç½®ï¼š
```python
config = {
    'min_length': 1,          # å…è®¸æ›´çŸ­
    'min_chinese_ratio': 0.1, # å…è®¸æ›´å¤šè‹±æ–‡
    'min_quality_score': 0.2, # é™ä½è´¨é‡è¦æ±‚
}
```

### Q4: å»é‡å¤ªæ…¢ï¼Ÿ
**A:**
1. å…ˆä½¿ç”¨ `exact_dedup`ï¼ˆæœ€å¿«ï¼‰
2. å‡å°æ•°æ®é‡å†ç”¨ `embedding_dedup`
3. è°ƒæ•´ `batch_size` å‚æ•°

---

## æ–‡ä»¶ä½ç½®

- **ä¸»è¦ä»£ç **: `analysis/services.py`
- **æµ‹è¯•è„šæœ¬**: `analysis/test_cleaning.py`
- **ä¾èµ–æ–‡ä»¶**: `requirements.txt`
- **å®Œæ•´æŒ‡å—**: `DATA_CLEANING_COMPLETE_GUIDE.md`ï¼ˆæœ¬æ–‡ä»¶ï¼‰

---

## ç‰ˆæœ¬å†å²

- **v3.0 (é˜¶æ®µä¸‰)**: æ·»åŠ Embeddingè¯­ä¹‰å»é‡ã€å®Œæ•´Pipelineç±»
- **v2.0 (é˜¶æ®µäºŒ)**: æ·»åŠ ç²¾ç¡®/æ¨¡ç³Šå»é‡ã€è´¨é‡è¯„åˆ†ç³»ç»Ÿ
- **v1.0 (é˜¶æ®µä¸€)**: åŸºç¡€æ¸…æ´—ã€Unicodeæ ‡å‡†åŒ–ã€ç¹ç®€è½¬æ¢

---

## ä¸‹ä¸€æ­¥å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒè‡ªå®šä¹‰å»é‡ç­–ç•¥
- [ ] æ·»åŠ æ›´å¤šè´¨é‡è¯„åˆ†ç»´åº¦
- [ ] æ€§èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–
- [ ] Webç•Œé¢å¯è§†åŒ–

---

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·åœ¨é¡¹ç›®ä¸­æå‡ºIssueã€‚
